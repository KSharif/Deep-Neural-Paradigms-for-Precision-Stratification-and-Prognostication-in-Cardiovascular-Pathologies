{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5592f5de-399b-4968-9f1b-10b04ca5c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /opt/anaconda3/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/anaconda3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df823b6-674b-4188-b8cd-d6921bb59381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /opt/anaconda3/lib/python3.12/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/anaconda3/lib/python3.12/site-packages (from ucimlrepo) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7294a9-b54e-4a9c-9fd3-b8b4e34d0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91        17\n",
      "           1       0.93      0.89      0.91        28\n",
      "           2       1.00      0.94      0.97        32\n",
      "           3       0.91      1.00      0.95        20\n",
      "           4       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.95       131\n",
      "   macro avg       0.94      0.95      0.95       131\n",
      "weighted avg       0.96      0.95      0.95       131\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91        17\n",
      "           1       0.96      0.96      0.96        28\n",
      "           2       0.97      1.00      0.98        32\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.98       131\n",
      "   macro avg       0.97      0.97      0.97       131\n",
      "weighted avg       0.98      0.98      0.98       131\n",
      "\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89        17\n",
      "           1       0.96      0.82      0.88        28\n",
      "           2       0.97      0.94      0.95        32\n",
      "           3       0.83      0.95      0.88        20\n",
      "           4       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.93       131\n",
      "   macro avg       0.92      0.93      0.92       131\n",
      "weighted avg       0.94      0.93      0.93       131\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85        17\n",
      "           1       0.88      0.82      0.85        28\n",
      "           2       0.94      0.91      0.92        32\n",
      "           3       0.86      0.95      0.90        20\n",
      "           4       0.94      1.00      0.97        34\n",
      "\n",
      "    accuracy                           0.91       131\n",
      "   macro avg       0.90      0.90      0.90       131\n",
      "weighted avg       0.91      0.91      0.91       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Import necessary libraries\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Step 2: Fetch and prepare the dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets\n",
    "\n",
    "# Combine X and y for exploration\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X_clean = df.drop(columns=['num'])  # Assuming 'num' is the target column\n",
    "y_clean = df['num']\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Polynomial Features for interactions\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Feature Selection using Random Forest\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "X_selected = selector.fit_transform(X_poly, y_clean)\n",
    "\n",
    "# Step 4: Handle Imbalance using SMOTEENN\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X_selected, y_clean)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Define and Tune Models\n",
    "\n",
    "# Random Forest Tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_tuned_model = RandomizedSearchCV(rf_model, rf_params, n_iter=30, cv=5, scoring='accuracy', random_state=42)\n",
    "rf_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_best_model = rf_tuned_model.best_estimator_\n",
    "y_pred_rf = rf_best_model.predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# SVM Tuning\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "svm_model = SVC(class_weight='balanced', probability=True, random_state=42)\n",
    "svm_tuned_model = RandomizedSearchCV(svm_model, svm_params, n_iter=30, cv=5, scoring='accuracy', random_state=42)\n",
    "svm_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_best_model = svm_tuned_model.best_estimator_\n",
    "y_pred_svm = svm_best_model.predict(X_test)\n",
    "print(\"SVM\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# XGBoost Tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb_tuned_model = RandomizedSearchCV(xgb_model, xgb_params, n_iter=30, cv=5, scoring='accuracy', random_state=42)\n",
    "xgb_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_best_model = xgb_tuned_model.best_estimator_\n",
    "y_pred_xgb = xgb_best_model.predict(X_test)\n",
    "print(\"XGBoost\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938beb35-a0a5-4a5f-b6cb-0011d68bad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        17\n",
      "           1       0.90      1.00      0.95        28\n",
      "           2       1.00      1.00      1.00        32\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.98       131\n",
      "   macro avg       0.98      0.96      0.97       131\n",
      "weighted avg       0.98      0.98      0.98       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "# K-Nearest Neighbors (KNN)\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_tuned_model = RandomizedSearchCV(knn_model, knn_params, n_iter=30, cv=5, scoring='accuracy', random_state=42)\n",
    "knn_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate KNN\n",
    "knn_best_model = knn_tuned_model.best_estimator_\n",
    "y_pred_knn = knn_best_model.predict(X_test)\n",
    "print(\"K-Nearest Neighbors\")\n",
    "print(classification_report(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98790600-a649-494f-8e77-db27ce0a2189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Enhanced Preprocessing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.58      0.58        12\n",
      "           1       0.43      0.86      0.57        28\n",
      "           2       0.89      0.27      0.41        30\n",
      "           3       0.78      0.56      0.65        25\n",
      "           4       0.80      0.80      0.80        30\n",
      "\n",
      "    accuracy                           0.62       125\n",
      "   macro avg       0.70      0.61      0.60       125\n",
      "weighted avg       0.71      0.62      0.60       125\n",
      "\n",
      "Decision Tree with Enhanced Preprocessing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.81      0.61      0.69        28\n",
      "           2       0.77      0.80      0.79        30\n",
      "           3       0.79      0.76      0.78        25\n",
      "           4       0.72      0.93      0.81        30\n",
      "\n",
      "    accuracy                           0.77       125\n",
      "   macro avg       0.78      0.75      0.76       125\n",
      "weighted avg       0.77      0.77      0.76       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Enhanced Preprocessing\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Adding Polynomial Features for interactions (degree = 3 for more interaction depth)\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Use PCA to reduce dimensionality (keeping 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_poly)\n",
    "\n",
    "# Feature Selection using Random Forest to reduce noise and redundant features\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "X_selected = selector.fit_transform(X_pca, y_clean)\n",
    "\n",
    "# Handle Imbalance using SMOTEENN\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X_selected, y_clean)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Naive Bayes with Enhanced Preprocessing\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "print(\"Naive Bayes with Enhanced Preprocessing\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Decision Tree with Hyperparameter Tuning\n",
    "dt_params = {\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt_tuned_model = RandomizedSearchCV(dt_model, dt_params, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "dt_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "dt_best_model = dt_tuned_model.best_estimator_\n",
    "y_pred_dt = dt_best_model.predict(X_test)\n",
    "print(\"Decision Tree with Enhanced Preprocessing\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fee07d97-766f-468e-a89b-3d1ef3c00c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 - 1s - 59ms/step - accuracy: 0.3531 - loss: 1.7812 - val_accuracy: 0.4811 - val_loss: 1.4764 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.5379 - loss: 1.1883 - val_accuracy: 0.5283 - val_loss: 1.3977 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.6469 - loss: 0.8700 - val_accuracy: 0.5755 - val_loss: 1.3560 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.7607 - loss: 0.7256 - val_accuracy: 0.5094 - val_loss: 1.3193 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.7915 - loss: 0.5685 - val_accuracy: 0.5000 - val_loss: 1.2960 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8341 - loss: 0.5002 - val_accuracy: 0.5189 - val_loss: 1.2588 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8412 - loss: 0.4828 - val_accuracy: 0.5472 - val_loss: 1.2034 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8697 - loss: 0.3871 - val_accuracy: 0.5755 - val_loss: 1.1439 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8910 - loss: 0.3599 - val_accuracy: 0.6226 - val_loss: 1.0888 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9028 - loss: 0.3329 - val_accuracy: 0.6604 - val_loss: 1.0377 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9171 - loss: 0.2627 - val_accuracy: 0.6792 - val_loss: 0.9796 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9076 - loss: 0.2757 - val_accuracy: 0.7170 - val_loss: 0.9264 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9147 - loss: 0.2591 - val_accuracy: 0.7358 - val_loss: 0.9097 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9028 - loss: 0.3033 - val_accuracy: 0.7075 - val_loss: 0.8706 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8886 - loss: 0.3442 - val_accuracy: 0.7075 - val_loss: 0.8401 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8957 - loss: 0.3407 - val_accuracy: 0.7453 - val_loss: 0.7684 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.8981 - loss: 0.3053 - val_accuracy: 0.7453 - val_loss: 0.6862 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9289 - loss: 0.2575 - val_accuracy: 0.7736 - val_loss: 0.6393 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9242 - loss: 0.2290 - val_accuracy: 0.8208 - val_loss: 0.6221 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "14/14 - 0s - 8ms/step - accuracy: 0.9218 - loss: 0.2443 - val_accuracy: 0.8396 - val_loss: 0.5529 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9573 - loss: 0.1678 - val_accuracy: 0.8396 - val_loss: 0.5092 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9479 - loss: 0.1674 - val_accuracy: 0.8585 - val_loss: 0.4490 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9455 - loss: 0.1808 - val_accuracy: 0.8585 - val_loss: 0.3997 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9597 - loss: 0.1351 - val_accuracy: 0.8679 - val_loss: 0.3530 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9479 - loss: 0.1618 - val_accuracy: 0.8679 - val_loss: 0.3319 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9645 - loss: 0.1324 - val_accuracy: 0.8962 - val_loss: 0.3182 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9550 - loss: 0.1321 - val_accuracy: 0.9057 - val_loss: 0.3032 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9621 - loss: 0.1357 - val_accuracy: 0.9151 - val_loss: 0.2751 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9668 - loss: 0.1198 - val_accuracy: 0.9245 - val_loss: 0.2559 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9834 - loss: 0.0940 - val_accuracy: 0.9245 - val_loss: 0.2537 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9763 - loss: 0.1051 - val_accuracy: 0.9245 - val_loss: 0.2504 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9692 - loss: 0.1121 - val_accuracy: 0.9057 - val_loss: 0.2546 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9573 - loss: 0.1286 - val_accuracy: 0.9057 - val_loss: 0.2413 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9763 - loss: 0.1109 - val_accuracy: 0.9151 - val_loss: 0.2392 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9573 - loss: 0.1295 - val_accuracy: 0.9057 - val_loss: 0.2450 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9597 - loss: 0.1237 - val_accuracy: 0.8962 - val_loss: 0.2470 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9834 - loss: 0.0829 - val_accuracy: 0.9057 - val_loss: 0.2356 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9621 - loss: 0.1303 - val_accuracy: 0.9245 - val_loss: 0.2328 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "14/14 - 0s - 9ms/step - accuracy: 0.9716 - loss: 0.0983 - val_accuracy: 0.9151 - val_loss: 0.2205 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9573 - loss: 0.1428 - val_accuracy: 0.9151 - val_loss: 0.2054 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9810 - loss: 0.1009 - val_accuracy: 0.9245 - val_loss: 0.2062 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9668 - loss: 0.1014 - val_accuracy: 0.9245 - val_loss: 0.2087 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9668 - loss: 0.0989 - val_accuracy: 0.9340 - val_loss: 0.2099 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9858 - loss: 0.0671 - val_accuracy: 0.9340 - val_loss: 0.2058 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9692 - loss: 0.1003 - val_accuracy: 0.9151 - val_loss: 0.2328 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9763 - loss: 0.0996 - val_accuracy: 0.9434 - val_loss: 0.2122 - learning_rate: 2.0000e-04\n",
      "Epoch 47/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9858 - loss: 0.0712 - val_accuracy: 0.9434 - val_loss: 0.2084 - learning_rate: 2.0000e-04\n",
      "Epoch 48/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9645 - loss: 0.1111 - val_accuracy: 0.9434 - val_loss: 0.2021 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9810 - loss: 0.0807 - val_accuracy: 0.9434 - val_loss: 0.1991 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9834 - loss: 0.0692 - val_accuracy: 0.9434 - val_loss: 0.2018 - learning_rate: 2.0000e-04\n",
      "Epoch 51/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9787 - loss: 0.0706 - val_accuracy: 0.9340 - val_loss: 0.2012 - learning_rate: 2.0000e-04\n",
      "Epoch 52/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9858 - loss: 0.0653 - val_accuracy: 0.9340 - val_loss: 0.2031 - learning_rate: 2.0000e-04\n",
      "Epoch 53/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9882 - loss: 0.0676 - val_accuracy: 0.9245 - val_loss: 0.2085 - learning_rate: 2.0000e-04\n",
      "Epoch 54/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9597 - loss: 0.1360 - val_accuracy: 0.9245 - val_loss: 0.2063 - learning_rate: 2.0000e-04\n",
      "Epoch 55/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9834 - loss: 0.0609 - val_accuracy: 0.9245 - val_loss: 0.2059 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "14/14 - 0s - 6ms/step - accuracy: 0.9787 - loss: 0.0793 - val_accuracy: 0.9245 - val_loss: 0.2060 - learning_rate: 4.0000e-05\n",
      "Epoch 57/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9763 - loss: 0.0843 - val_accuracy: 0.9245 - val_loss: 0.2051 - learning_rate: 4.0000e-05\n",
      "Epoch 58/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9716 - loss: 0.0948 - val_accuracy: 0.9245 - val_loss: 0.2045 - learning_rate: 4.0000e-05\n",
      "Epoch 59/100\n",
      "14/14 - 0s - 7ms/step - accuracy: 0.9763 - loss: 0.0878 - val_accuracy: 0.9245 - val_loss: 0.2053 - learning_rate: 4.0000e-05\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9687 - loss: 0.0926\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x2b5c8fd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x2b5c8fd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "CNN Model Test Accuracy: 96.21%\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93        22\n",
      "           1       0.96      0.96      0.96        24\n",
      "           2       0.91      1.00      0.95        31\n",
      "           3       1.00      0.92      0.96        24\n",
      "           4       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.96       132\n",
      "   macro avg       0.96      0.96      0.96       132\n",
      "weighted avg       0.96      0.96      0.96       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.combine import SMOTEENN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 2: Fetch and prepare the dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets\n",
    "\n",
    "# Combine X and y for exploration\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X_clean = df.drop(columns=['num'])  # Assuming 'num' is the target column\n",
    "y_clean = df['num']\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Polynomial Features for interactions\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Handle Imbalance using SMOTEENN\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X_poly, y_clean)\n",
    "\n",
    "# Reshape data for CNN (as CNN expects a 3D shape: [samples, time steps, features])\n",
    "X_resampled_reshaped = X_resampled.reshape(X_resampled.shape[0], X_resampled.shape[1], 1)\n",
    "\n",
    "# Convert target to categorical (for multiclass classification)\n",
    "y_resampled_categorical = to_categorical(y_resampled)\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_reshaped, y_resampled_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Define CNN Model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# 1D Convolution Layer\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "cnn_model.add(BatchNormalization())  # Batch normalization for faster convergence\n",
    "\n",
    "# Max Pooling Layer\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Additional Conv Layer to increase capacity\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten layer\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Fully Connected Dense Layers\n",
    "cnn_model.add(Dense(256, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))  # Regularization\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer (for multiclass classification)\n",
    "cnn_model.add(Dense(y_resampled_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 7: Set callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# Step 8: Train the CNN Model\n",
    "history = cnn_model.fit(X_train, y_train, \n",
    "                        epochs=100, \n",
    "                        validation_split=0.2, \n",
    "                        batch_size=32, \n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=2)\n",
    "\n",
    "# Step 9: Evaluate the CNN Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predict and calculate classification report\n",
    "y_pred_cnn = cnn_model.predict(X_test)\n",
    "y_pred_cnn_classes = np.argmax(y_pred_cnn, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Step 10: Display classification report\n",
    "print(f\"CNN Model Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_classes, y_pred_cnn_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd1fe8cb-b5c4-493e-b909-14a3c1adf789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.0212\n",
      "Epoch 20/50, Loss: 0.0045\n",
      "Epoch 30/50, Loss: 0.0069\n",
      "Epoch 40/50, Loss: 0.0026\n",
      "Epoch 50/50, Loss: 0.0006\n",
      "Transformer Test Accuracy: 95.45%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Step 2: Define the Transformer Model with Dynamic Positional Encoding for Tabular Data\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=64, nhead=4, num_layers=2, dim_feedforward=128):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Define Transformer Encoder Layer with batch_first=True for improved performance\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Reshape to add sequence dimension, making it compatible with Transformer\n",
    "        x = x.unsqueeze(1)  # Shape becomes (batch_size, 1, d_model)\n",
    "        \n",
    "        # Dynamic positional encoding\n",
    "        pos_encoder = torch.zeros(x.size()).to(x.device)\n",
    "        x = x + pos_encoder  # Add positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # Apply transformer\n",
    "        x = x.mean(dim=1)  # Pooling across the \"sequence\" dimension\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate and test model\n",
    "model = TabularTransformer(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Step 3: Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Evaluation\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Transformer Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8885eed4-664a-46f2-bf28-55cfc4b32252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0146\n",
      "Epoch 20/100, Loss: 0.0064\n",
      "Epoch 30/100, Loss: 0.0038\n",
      "Epoch 40/100, Loss: 0.0018\n",
      "Epoch 50/100, Loss: 0.0013\n",
      "Epoch 60/100, Loss: 0.0016\n",
      "Epoch 70/100, Loss: 0.0028\n",
      "Epoch 80/100, Loss: 0.1013\n",
      "Epoch 90/100, Loss: 0.0010\n",
      "Epoch 100/100, Loss: 0.0010\n",
      "Enhanced Transformer Test Accuracy: 98.48%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "# Updated Transformer Model for Improved Performance\n",
    "class EnhancedTabularTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.3):\n",
    "        super(EnhancedTabularTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Updated Transformer Encoder with additional layers and dropout for regularization\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for Transformer compatibility\n",
    "        \n",
    "        pos_encoder = torch.zeros(x.size()).to(x.device)  # Dynamic positional encoding\n",
    "        x = x + pos_encoder  # Apply positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # Transformer processing\n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate model with increased dimensions\n",
    "model = EnhancedTabularTransformer(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# Redefine the optimizer with a lower learning rate for finer tuning\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Increased epochs for training\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the updated model\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Enhanced Transformer Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eebb4471-6987-4262-b7a2-9bed5b25ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Accuracy: 97.73%\n",
      "Early stopping!\n",
      "Best Validation Accuracy Achieved: 98.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/fvm8bd317t9f9_j96tkt0q800000gn/T/ipykernel_75630/2839455758.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Define optimizer with a scheduler and early stopping patience\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "# Early stopping parameters\n",
    "best_accuracy = 0\n",
    "patience, trigger_times = 10, 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation accuracy check for early stopping\n",
    "    model.eval()\n",
    "    y_val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_val_pred.extend(predicted.numpy())\n",
    "    val_accuracy = accuracy_score(y_test, y_val_pred)\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        trigger_times = 0  # Reset counter\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(f\"Best Validation Accuracy Achieved: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c29d7a6-219c-4309-b5d2-893712123e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6932\n",
      "Epoch [2/100], Loss: 0.9888\n",
      "Epoch [3/100], Loss: 0.6625\n",
      "Epoch [4/100], Loss: 0.6909\n",
      "Epoch [5/100], Loss: 0.7381\n",
      "Epoch [6/100], Loss: 0.7441\n",
      "Epoch [7/100], Loss: 0.6555\n",
      "Epoch [8/100], Loss: 0.8668\n",
      "Epoch [9/100], Loss: 0.7201\n",
      "Epoch [10/100], Loss: 0.6865\n",
      "Epoch [11/100], Loss: 0.6979\n",
      "Epoch [12/100], Loss: 0.7167\n",
      "Epoch [13/100], Loss: 0.7469\n",
      "Epoch [14/100], Loss: 0.6868\n",
      "Epoch [15/100], Loss: 0.6918\n",
      "Epoch [16/100], Loss: 0.7036\n",
      "Epoch [17/100], Loss: 0.6710\n",
      "Epoch [18/100], Loss: 0.7021\n",
      "Epoch [19/100], Loss: 0.7564\n",
      "Epoch [20/100], Loss: 0.7781\n",
      "Epoch [21/100], Loss: 0.7480\n",
      "Epoch [22/100], Loss: 0.6932\n",
      "Epoch [23/100], Loss: 0.6994\n",
      "Epoch [24/100], Loss: 0.7573\n",
      "Epoch [25/100], Loss: 0.6982\n",
      "Epoch [26/100], Loss: 0.6981\n",
      "Epoch [27/100], Loss: 0.7046\n",
      "Epoch [28/100], Loss: 0.7080\n",
      "Epoch [29/100], Loss: 0.6930\n",
      "Epoch [30/100], Loss: 0.6720\n",
      "Epoch [31/100], Loss: 0.7003\n",
      "Epoch [32/100], Loss: 0.6986\n",
      "Epoch [33/100], Loss: 0.7049\n",
      "Epoch [34/100], Loss: 0.6930\n",
      "Epoch [35/100], Loss: 0.6919\n",
      "Epoch [36/100], Loss: 0.7216\n",
      "Epoch [37/100], Loss: 0.6201\n",
      "Epoch [38/100], Loss: 0.7441\n",
      "Epoch [39/100], Loss: 0.7774\n",
      "Epoch [40/100], Loss: 0.6986\n",
      "Epoch [41/100], Loss: 0.6934\n",
      "Epoch [42/100], Loss: 0.6907\n",
      "Epoch [43/100], Loss: 0.6888\n",
      "Epoch [44/100], Loss: 0.7100\n",
      "Epoch [45/100], Loss: 0.6945\n",
      "Epoch [46/100], Loss: 0.7000\n",
      "Epoch [47/100], Loss: 0.6772\n",
      "Epoch [48/100], Loss: 0.6631\n",
      "Epoch [49/100], Loss: 0.5376\n",
      "Epoch [50/100], Loss: 0.7280\n",
      "Epoch [51/100], Loss: 0.7093\n",
      "Epoch [52/100], Loss: 0.8864\n",
      "Epoch [53/100], Loss: 0.6883\n",
      "Epoch [54/100], Loss: 0.6828\n",
      "Epoch [55/100], Loss: 0.6962\n",
      "Epoch [56/100], Loss: 0.6931\n",
      "Epoch [57/100], Loss: 0.6951\n",
      "Epoch [58/100], Loss: 0.6971\n",
      "Epoch [59/100], Loss: 0.6946\n",
      "Epoch [60/100], Loss: 0.6917\n",
      "Epoch [61/100], Loss: 0.6924\n",
      "Epoch [62/100], Loss: 0.6900\n",
      "Epoch [63/100], Loss: 0.6867\n",
      "Epoch [64/100], Loss: 0.7005\n",
      "Epoch [65/100], Loss: 0.7027\n",
      "Epoch [66/100], Loss: 0.6963\n",
      "Epoch [67/100], Loss: 0.6799\n",
      "Epoch [68/100], Loss: 0.6786\n",
      "Epoch [69/100], Loss: 0.6794\n",
      "Epoch [70/100], Loss: 0.6763\n",
      "Epoch [71/100], Loss: 0.8148\n",
      "Epoch [72/100], Loss: 0.6994\n",
      "Epoch [73/100], Loss: 0.7006\n",
      "Epoch [74/100], Loss: 0.7030\n",
      "Epoch [75/100], Loss: 0.7058\n",
      "Epoch [76/100], Loss: 0.7149\n",
      "Epoch [77/100], Loss: 0.7144\n",
      "Epoch [78/100], Loss: 0.7041\n",
      "Epoch [79/100], Loss: 0.6985\n",
      "Epoch [80/100], Loss: 0.6913\n",
      "Epoch [81/100], Loss: 0.7003\n",
      "Epoch [82/100], Loss: 0.6671\n",
      "Epoch [83/100], Loss: 0.7237\n",
      "Epoch [84/100], Loss: 0.6971\n",
      "Epoch [85/100], Loss: 0.6952\n",
      "Epoch [86/100], Loss: 0.6850\n",
      "Epoch [87/100], Loss: 0.6989\n",
      "Epoch [88/100], Loss: 0.6932\n",
      "Epoch [89/100], Loss: 0.6932\n",
      "Epoch [90/100], Loss: 0.6905\n",
      "Epoch [91/100], Loss: 0.6965\n",
      "Epoch [92/100], Loss: 0.7032\n",
      "Epoch [93/100], Loss: 0.6916\n",
      "Epoch [94/100], Loss: 0.6893\n",
      "Epoch [95/100], Loss: 0.6919\n",
      "Epoch [96/100], Loss: 0.7036\n",
      "Epoch [97/100], Loss: 0.6902\n",
      "Epoch [98/100], Loss: 0.6967\n",
      "Epoch [99/100], Loss: 0.6917\n",
      "Epoch [100/100], Loss: 0.6864\n",
      "Sample Prediction: [[0.52025217]\n",
      " [0.52025384]\n",
      " [0.52025276]\n",
      " [0.52025074]\n",
      " [0.52024794]\n",
      " [0.52024925]\n",
      " [0.52025014]\n",
      " [0.52025074]\n",
      " [0.52024925]\n",
      " [0.52025163]\n",
      " [0.52025115]\n",
      " [0.52025133]\n",
      " [0.5202522 ]\n",
      " [0.5202489 ]\n",
      " [0.5202524 ]\n",
      " [0.52025175]\n",
      " [0.5202526 ]\n",
      " [0.5202499 ]\n",
      " [0.5202517 ]\n",
      " [0.52025133]\n",
      " [0.520251  ]\n",
      " [0.5202491 ]\n",
      " [0.52025145]\n",
      " [0.5202506 ]\n",
      " [0.52025247]\n",
      " [0.5202508 ]\n",
      " [0.520249  ]\n",
      " [0.5202511 ]\n",
      " [0.520252  ]\n",
      " [0.5202532 ]\n",
      " [0.5202523 ]\n",
      " [0.5202505 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Initialize tokenizer for clinical text processing\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define CNN for ECG Data\n",
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(16 * 49, 128)  # Output 128-dim features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define Transformer Encoder Layer with Attention Gating for Tabular Data\n",
    "class BidirectionalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BidirectionalTransformer, self).__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=4, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, 0, :])  # CLS token as summary\n",
    "\n",
    "# BERT Model for Text Data\n",
    "class ClinicalTextBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClinicalTextBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x).last_hidden_state[:, 0, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "# Cross-Attention Mechanism for Fusion\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=384, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Linear(384, 128)\n",
    "\n",
    "    def forward(self, ecg, tabular, text):\n",
    "        # Concatenate along feature dimension to ensure 384-dim input to attention\n",
    "        combined = torch.cat([ecg.unsqueeze(1), tabular.unsqueeze(1), text.unsqueeze(1)], dim=-1)\n",
    "        combined = combined.permute(1, 0, 2)  # Format for attention (seq_len, batch, embed_dim)\n",
    "        \n",
    "        # Multihead Attention\n",
    "        attention_output, _ = self.cross_attention(combined, combined, combined)\n",
    "        \n",
    "        # Flatten and pass through final fully connected layer\n",
    "        attention_output = attention_output.mean(dim=0)  # (batch, embed_dim)\n",
    "        return self.fc(attention_output)\n",
    "\n",
    "# Complete Hybrid Model: CNN-BiTAM\n",
    "class CNN_BiTAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiTAM, self).__init__()\n",
    "        self.ecg_cnn = ECG_CNN()\n",
    "        self.tabular_transformer = BidirectionalTransformer(128)\n",
    "        self.text_bert = ClinicalTextBERT()\n",
    "        self.fusion_layer = CrossAttentionFusion()\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, ecg, tabular, text):\n",
    "        ecg_features = self.ecg_cnn(ecg)\n",
    "        tabular_features = self.tabular_transformer(tabular)\n",
    "        text_features = self.text_bert(text)\n",
    "        \n",
    "        fused_features = self.fusion_layer(ecg_features, tabular_features, text_features)\n",
    "        return torch.sigmoid(self.classifier(fused_features))\n",
    "\n",
    "# Sample Data Preparation\n",
    "ecg_data = torch.rand(32, 1, 100)         # ECG data (batch_size, channels, length)\n",
    "tabular_data = torch.rand(32, 10, 128)    # Tabular data (batch_size, seq_length, feature_dim)\n",
    "text_input = [\"Patient has a history of chest pain and hypertension.\"] * 32\n",
    "encoded_text = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "text_data = encoded_text['input_ids']\n",
    "\n",
    "# Instantiate Model\n",
    "model = CNN_BiTAM()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Example\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(ecg_data, tabular_data, text_data)\n",
    "    target = torch.rand(32, 1).round()  # Example target (random binary labels)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Model Output\n",
    "print(\"Sample Prediction:\", output.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8db59cc6-ff8b-409c-b138-26271a18a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6867, Val Loss: 0.6942, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.7377, Val Loss: 0.7913, Accuracy: 0.3462, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.7727, Val Loss: 0.7126, Accuracy: 0.3462, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [4/100], Loss: 0.7018, Val Loss: 0.7151, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [5/100], Loss: 0.7747, Val Loss: 0.6576, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [6/100], Loss: 0.6708, Val Loss: 0.6602, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [7/100], Loss: 0.6733, Val Loss: 0.6454, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [8/100], Loss: 0.6639, Val Loss: 0.6795, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [9/100], Loss: 0.6815, Val Loss: 0.6451, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [10/100], Loss: 0.6690, Val Loss: 0.6500, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [11/100], Loss: 0.6637, Val Loss: 0.6457, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [12/100], Loss: 0.6741, Val Loss: 0.6693, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [13/100], Loss: 0.6766, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [14/100], Loss: 0.6681, Val Loss: 0.6452, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [15/100], Loss: 0.6781, Val Loss: 0.6523, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [16/100], Loss: 0.6735, Val Loss: 0.6640, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [17/100], Loss: 0.6603, Val Loss: 0.6548, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [18/100], Loss: 0.6664, Val Loss: 0.6504, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [19/100], Loss: 0.6757, Val Loss: 0.6469, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [20/100], Loss: 0.6701, Val Loss: 0.6456, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [21/100], Loss: 0.6779, Val Loss: 0.6452, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [22/100], Loss: 0.6797, Val Loss: 0.6452, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [23/100], Loss: 0.6575, Val Loss: 0.6455, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [24/100], Loss: 0.6650, Val Loss: 0.6461, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [25/100], Loss: 0.6705, Val Loss: 0.6473, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [26/100], Loss: 0.6694, Val Loss: 0.6480, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [27/100], Loss: 0.6739, Val Loss: 0.6486, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [28/100], Loss: 0.6762, Val Loss: 0.6492, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [29/100], Loss: 0.6710, Val Loss: 0.6496, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [30/100], Loss: 0.6692, Val Loss: 0.6497, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [31/100], Loss: 0.6658, Val Loss: 0.6496, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [32/100], Loss: 0.6629, Val Loss: 0.6495, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [33/100], Loss: 0.6739, Val Loss: 0.6493, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [34/100], Loss: 0.6698, Val Loss: 0.6492, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [35/100], Loss: 0.6690, Val Loss: 0.6491, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [36/100], Loss: 0.6630, Val Loss: 0.6490, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [37/100], Loss: 0.6709, Val Loss: 0.6489, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [38/100], Loss: 0.6712, Val Loss: 0.6489, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [39/100], Loss: 0.6667, Val Loss: 0.6488, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [40/100], Loss: 0.6795, Val Loss: 0.6487, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [41/100], Loss: 0.6684, Val Loss: 0.6486, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [42/100], Loss: 0.6644, Val Loss: 0.6486, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [43/100], Loss: 0.6615, Val Loss: 0.6485, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [44/100], Loss: 0.6683, Val Loss: 0.6485, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [45/100], Loss: 0.6706, Val Loss: 0.6484, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [46/100], Loss: 0.6702, Val Loss: 0.6484, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [47/100], Loss: 0.6720, Val Loss: 0.6484, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [48/100], Loss: 0.6596, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [49/100], Loss: 0.6657, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [50/100], Loss: 0.6671, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [51/100], Loss: 0.6654, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [52/100], Loss: 0.6668, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [53/100], Loss: 0.6715, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [54/100], Loss: 0.6621, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [55/100], Loss: 0.6660, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [56/100], Loss: 0.6617, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [57/100], Loss: 0.6575, Val Loss: 0.6483, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [58/100], Loss: 0.6706, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [59/100], Loss: 0.6615, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [60/100], Loss: 0.6596, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [61/100], Loss: 0.6644, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [62/100], Loss: 0.6627, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [63/100], Loss: 0.6547, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [64/100], Loss: 0.6564, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [65/100], Loss: 0.6704, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [66/100], Loss: 0.6718, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [67/100], Loss: 0.6693, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [68/100], Loss: 0.6701, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [69/100], Loss: 0.6754, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [70/100], Loss: 0.6696, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [71/100], Loss: 0.6668, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [72/100], Loss: 0.6604, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [73/100], Loss: 0.6586, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [74/100], Loss: 0.6699, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [75/100], Loss: 0.6734, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [76/100], Loss: 0.6630, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [77/100], Loss: 0.6630, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [78/100], Loss: 0.6734, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [79/100], Loss: 0.6678, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [80/100], Loss: 0.6722, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [81/100], Loss: 0.6677, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [82/100], Loss: 0.6628, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [83/100], Loss: 0.6612, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [84/100], Loss: 0.6702, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [85/100], Loss: 0.6713, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [86/100], Loss: 0.6702, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [87/100], Loss: 0.6685, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [88/100], Loss: 0.6630, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [89/100], Loss: 0.6656, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [90/100], Loss: 0.6658, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [91/100], Loss: 0.6541, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [92/100], Loss: 0.6736, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [93/100], Loss: 0.6639, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [94/100], Loss: 0.6663, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [95/100], Loss: 0.6657, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [96/100], Loss: 0.6665, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [97/100], Loss: 0.6687, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [98/100], Loss: 0.6669, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [99/100], Loss: 0.6645, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Epoch [100/100], Loss: 0.6669, Val Loss: 0.6482, Accuracy: 0.6538, Precision: 0.6538, Recall: 1.0000, F1 Score: 0.7907\n",
      "Training completed. Evaluating final model performance...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenizer initialization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Preprocessing: Standardize Tabular Data and Prepare Text\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Sample Data Preparation\n",
    "ecg_data = torch.rand(128, 1, 100)  # ECG data (batch_size, channels, length)\n",
    "tabular_data = scaler.fit_transform(torch.rand(128, 10, 128).reshape(-1, 128)).reshape(128, 10, 128)\n",
    "tabular_data = torch.tensor(tabular_data, dtype=torch.float32)\n",
    "text_input = [\"Patient has a history of chest pain and hypertension.\"] * 128\n",
    "encoded_text = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "text_data = encoded_text['input_ids']\n",
    "\n",
    "# Train-Test Split\n",
    "ecg_train, ecg_val, tab_train, tab_val, text_train, text_val = train_test_split(\n",
    "    ecg_data, tabular_data, text_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define Enhanced CNN for ECG Data\n",
    "# Define Enhanced CNN for ECG Data\n",
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Dynamically determine the flattened input size\n",
    "        self._to_linear = None\n",
    "        self.init_flattened_size()\n",
    "        self.fc = nn.Linear(self._to_linear, 128)\n",
    "\n",
    "    def init_flattened_size(self):\n",
    "        # Pass a sample input through the layers to determine the output size\n",
    "        x = torch.rand(1, 1, 100)  # Sample input (batch_size, channels, length)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        self._to_linear = x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten dynamically based on calculated size\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Define Transformer with Extra Layers and Attention for Tabular Data\n",
    "class BidirectionalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BidirectionalTransformer, self).__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=8, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=3)\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, 0, :])\n",
    "\n",
    "# Text Processing with ClinicalTextBERT\n",
    "class ClinicalTextBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClinicalTextBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x).last_hidden_state[:, 0, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "# Cross-Attention Fusion Mechanism with More Layers\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=384, num_heads=8, batch_first=True)\n",
    "        self.fc = nn.Linear(384, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, ecg, tabular, text):\n",
    "        combined = torch.cat([ecg.unsqueeze(1), tabular.unsqueeze(1), text.unsqueeze(1)], dim=-1)\n",
    "        combined = combined.permute(1, 0, 2)\n",
    "        \n",
    "        attention_output, _ = self.cross_attention(combined, combined, combined)\n",
    "        attention_output = attention_output.mean(dim=0)\n",
    "        return self.dropout(self.fc(attention_output))\n",
    "\n",
    "# Updated Hybrid Model with Enhanced Components\n",
    "class CNN_BiTAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiTAM, self).__init__()\n",
    "        self.ecg_cnn = ECG_CNN()\n",
    "        self.tabular_transformer = BidirectionalTransformer(128)\n",
    "        self.text_bert = ClinicalTextBERT()\n",
    "        self.fusion_layer = CrossAttentionFusion()\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, ecg, tabular, text):\n",
    "        ecg_features = self.ecg_cnn(ecg)\n",
    "        tabular_features = self.tabular_transformer(tabular)\n",
    "        text_features = self.text_bert(text)\n",
    "        \n",
    "        fused_features = self.fusion_layer(ecg_features, tabular_features, text_features)\n",
    "        return torch.sigmoid(self.classifier(fused_features))\n",
    "\n",
    "# Model Initialization\n",
    "model = CNN_BiTAM()\n",
    "\n",
    "# Define weighted loss for class imbalance and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Ensure consistent batch size\n",
    "batch_size = 128\n",
    "target = torch.rand(batch_size, 1).round()  # Example target (random binary labels)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(outputs, targets):\n",
    "    outputs = (outputs > 0.5).float()  # Convert probabilities to binary predictions\n",
    "    targets = targets.float()\n",
    "    accuracy = accuracy_score(targets.cpu(), outputs.cpu())\n",
    "    precision = precision_score(targets.cpu(), outputs.cpu())\n",
    "    recall = recall_score(targets.cpu(), outputs.cpu())\n",
    "    f1 = f1_score(targets.cpu(), outputs.cpu())\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training Loop with Metrics Calculation\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "target = torch.rand(batch_size, 1).round()  # Example target (random binary labels)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Training Batch\n",
    "    ecg_batch = ecg_train[:batch_size]\n",
    "    tab_batch = tab_train[:batch_size]\n",
    "    text_batch = text_train[:batch_size]\n",
    "    \n",
    "    output = model(ecg_batch, tab_batch, text_batch)\n",
    "    \n",
    "    # Ensure target size matches output\n",
    "    target = target[:output.size(0)]\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ecg_val_batch = ecg_val[:batch_size]\n",
    "        tab_val_batch = tab_val[:batch_size]\n",
    "        text_val_batch = text_val[:batch_size]\n",
    "\n",
    "        val_output = model(ecg_val_batch, tab_val_batch, text_val_batch)\n",
    "        val_target = target[:val_output.size(0)]\n",
    "        val_loss = criterion(val_output, val_target)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(val_output, val_target)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "          f\"Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "          f\"Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Training completed. Evaluating final model performance...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e5f3778-40c1-4351-940b-babfd9406c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6780, Val Loss: 0.7290, Accuracy: 0.5300, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [2/100], Loss: 0.7906, Val Loss: 0.7135, Accuracy: 0.5200, Precision: 0.5200, Recall: 1.0000, F1 Score: 0.6842\n",
      "Epoch [3/100], Loss: 0.6767, Val Loss: 0.7054, Accuracy: 0.4800, Precision: 0.4800, Recall: 1.0000, F1 Score: 0.6486\n",
      "Epoch [4/100], Loss: 0.7062, Val Loss: 0.8634, Accuracy: 0.5100, Precision: 0.5100, Recall: 1.0000, F1 Score: 0.6755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.7204, Val Loss: 1.2983, Accuracy: 0.5900, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.7984, Val Loss: 0.8886, Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [7/100], Loss: 0.6162, Val Loss: 0.7009, Accuracy: 0.5300, Precision: 0.4884, Recall: 0.9333, F1 Score: 0.6412\n",
      "Epoch [8/100], Loss: 0.6890, Val Loss: 0.7033, Accuracy: 0.4800, Precision: 0.4800, Recall: 1.0000, F1 Score: 0.6486\n",
      "Epoch [9/100], Loss: 0.7592, Val Loss: 0.6955, Accuracy: 0.4500, Precision: 0.4388, Recall: 1.0000, F1 Score: 0.6099\n",
      "Epoch [10/100], Loss: 0.7235, Val Loss: 0.6961, Accuracy: 0.4900, Precision: 0.4842, Recall: 0.9583, F1 Score: 0.6434\n",
      "Epoch [11/100], Loss: 0.7536, Val Loss: 0.6958, Accuracy: 0.4900, Precision: 0.4792, Recall: 0.9787, F1 Score: 0.6434\n",
      "Epoch [12/100], Loss: 0.7749, Val Loss: 0.6852, Accuracy: 0.5700, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [13/100], Loss: 0.7558, Val Loss: 0.7066, Accuracy: 0.4900, Precision: 0.4900, Recall: 1.0000, F1 Score: 0.6577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.7983, Val Loss: 0.8558, Accuracy: 0.5200, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.6963, Val Loss: 0.7735, Accuracy: 0.5100, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [16/100], Loss: 0.6314, Val Loss: 0.7957, Accuracy: 0.4400, Precision: 0.4400, Recall: 1.0000, F1 Score: 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.7336, Val Loss: 0.7703, Accuracy: 0.4800, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [18/100], Loss: 0.6667, Val Loss: 0.9534, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1 Score: 0.6667\n",
      "Epoch [19/100], Loss: 0.7377, Val Loss: 0.7076, Accuracy: 0.4800, Precision: 0.4800, Recall: 1.0000, F1 Score: 0.6486\n",
      "Epoch [20/100], Loss: 0.7196, Val Loss: 0.6692, Accuracy: 0.6100, Precision: 0.6100, Recall: 1.0000, F1 Score: 0.7578\n",
      "Epoch [21/100], Loss: 0.7666, Val Loss: 0.7684, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1 Score: 0.6667\n",
      "Epoch [22/100], Loss: 0.6856, Val Loss: 0.6903, Accuracy: 0.5400, Precision: 0.5422, Recall: 0.8491, F1 Score: 0.6618\n",
      "Epoch [23/100], Loss: 0.6676, Val Loss: 0.6867, Accuracy: 0.5400, Precision: 0.4286, Recall: 0.0667, F1 Score: 0.1154\n",
      "Epoch [24/100], Loss: 0.7182, Val Loss: 0.6921, Accuracy: 0.5400, Precision: 0.5476, Recall: 0.8519, F1 Score: 0.6667\n",
      "Epoch [25/100], Loss: 0.6760, Val Loss: 0.6951, Accuracy: 0.4800, Precision: 0.5000, Recall: 0.2885, F1 Score: 0.3659\n",
      "Epoch [26/100], Loss: 0.7012, Val Loss: 0.7025, Accuracy: 0.4000, Precision: 0.4000, Recall: 0.1053, F1 Score: 0.1667\n",
      "Epoch [27/100], Loss: 0.6764, Val Loss: 0.6971, Accuracy: 0.4500, Precision: 0.4737, Recall: 0.1667, F1 Score: 0.2466\n",
      "Epoch [28/100], Loss: 0.7835, Val Loss: 0.6922, Accuracy: 0.5200, Precision: 0.4762, Recall: 0.2128, F1 Score: 0.2941\n",
      "Epoch [29/100], Loss: 0.6795, Val Loss: 0.6947, Accuracy: 0.4200, Precision: 0.3559, Recall: 0.5122, F1 Score: 0.4200\n",
      "Early stopping triggered.\n",
      "Training completed. Evaluating final model performance...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Tokenizer initialization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Preprocessing: Standardize Tabular Data and Prepare Text\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Sample Data Preparation\n",
    "ecg_data = torch.rand(500, 1, 100)  # Larger dataset (batch_size, channels, length)\n",
    "tabular_data = scaler.fit_transform(torch.rand(500, 10, 128).reshape(-1, 128)).reshape(500, 10, 128)\n",
    "tabular_data = torch.tensor(tabular_data, dtype=torch.float32)\n",
    "text_input = [\"Patient has a history of chest pain and hypertension.\"] * 500\n",
    "encoded_text = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "text_data = encoded_text['input_ids']\n",
    "\n",
    "# Train-Test Split\n",
    "ecg_train, ecg_val, tab_train, tab_val, text_train, text_val = train_test_split(\n",
    "    ecg_data, tabular_data, text_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define Enhanced CNN for ECG Data\n",
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self._to_linear = None\n",
    "        self.init_flattened_size()\n",
    "        self.fc = nn.Linear(self._to_linear, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "\n",
    "    def init_flattened_size(self):\n",
    "        x = torch.rand(1, 1, 100)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        self._to_linear = x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.bn(self.fc(x))\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define Transformer with Extra Layers and Attention for Tabular Data\n",
    "class BidirectionalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BidirectionalTransformer, self).__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=8, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=3)\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        return self.bn(self.fc(x[:, 0, :]))\n",
    "\n",
    "# Text Processing with ClinicalTextBERT\n",
    "class ClinicalTextBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClinicalTextBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x).last_hidden_state[:, 0, :]\n",
    "        return self.bn(self.fc(x))\n",
    "\n",
    "# Cross-Attention Fusion Mechanism with More Layers\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=384, num_heads=8, batch_first=True)\n",
    "        self.fc = nn.Linear(384, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, ecg, tabular, text):\n",
    "        combined = torch.cat([ecg.unsqueeze(1), tabular.unsqueeze(1), text.unsqueeze(1)], dim=-1)\n",
    "        combined = combined.permute(1, 0, 2)\n",
    "        attention_output, _ = self.cross_attention(combined, combined, combined)\n",
    "        attention_output = attention_output.mean(dim=0)\n",
    "        return self.dropout(self.fc(attention_output))\n",
    "\n",
    "# Updated Hybrid Model with Enhanced Components\n",
    "class CNN_BiTAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiTAM, self).__init__()\n",
    "        self.ecg_cnn = ECG_CNN()\n",
    "        self.tabular_transformer = BidirectionalTransformer(128)\n",
    "        self.text_bert = ClinicalTextBERT()\n",
    "        self.fusion_layer = CrossAttentionFusion()\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, ecg, tabular, text):\n",
    "        ecg_features = self.ecg_cnn(ecg)\n",
    "        tabular_features = self.tabular_transformer(tabular)\n",
    "        text_features = self.text_bert(text)\n",
    "        fused_features = self.fusion_layer(ecg_features, tabular_features, text_features)\n",
    "        return torch.sigmoid(self.classifier(fused_features))\n",
    "\n",
    "# Model Initialization\n",
    "model = CNN_BiTAM()\n",
    "\n",
    "# Define weighted loss for class imbalance and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# Training configurations\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(outputs, targets):\n",
    "    outputs = (outputs > 0.5).float()\n",
    "    targets = targets.float()\n",
    "    accuracy = accuracy_score(targets.cpu(), outputs.cpu())\n",
    "    precision = precision_score(targets.cpu(), outputs.cpu())\n",
    "    recall = recall_score(targets.cpu(), outputs.cpu())\n",
    "    f1 = f1_score(targets.cpu(), outputs.cpu())\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training Loop with Early Stopping and Metrics Calculation\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(ecg_train), batch_size):\n",
    "        ecg_batch = ecg_train[i:i+batch_size]\n",
    "        tab_batch = tab_train[i:i+batch_size]\n",
    "        text_batch = text_train[i:i+batch_size]\n",
    "        \n",
    "        output = model(ecg_batch, tab_batch, text_batch)\n",
    "        batch_target = torch.rand(output.size(0), 1).round()  # Simulated target\n",
    "        loss = criterion(output, batch_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(ecg_val, tab_val, text_val)\n",
    "        val_target = torch.rand(val_output.size(0), 1).round()\n",
    "        val_loss = criterion(val_output, val_target)\n",
    "        scheduler.step(epoch + val_loss)  # Adjust LR dynamically with epoch + val_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy, precision, recall, f1 = calculate_metrics(val_output, val_target)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "          f\"Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "          f\"Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Training completed. Evaluating final model performance...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dfc5a1b-f4b9-4afa-9a1f-0f0c425e8cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Accuracy: 87.18%\n",
      "Early stopping!\n",
      "Best Validation Accuracy for Fold 1: 87.18%\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Accuracy: 82.05%\n",
      "Epoch 20/100, Validation Accuracy: 89.74%\n",
      "Epoch 30/100, Validation Accuracy: 94.87%\n",
      "Early stopping!\n",
      "Best Validation Accuracy for Fold 2: 94.87%\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Accuracy: 89.74%\n",
      "Early stopping!\n",
      "Best Validation Accuracy for Fold 3: 94.87%\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Accuracy: 84.62%\n",
      "Early stopping!\n",
      "Best Validation Accuracy for Fold 4: 87.18%\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Accuracy: 87.18%\n",
      "Early stopping!\n",
      "Best Validation Accuracy for Fold 5: 87.18%\n",
      "\n",
      "Average Cross-Validation Accuracy: 90.26%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess the Parkinson's dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(['name', 'status'], axis=1)\n",
    "y = df['status'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.fc_in = nn.Linear(input_dim, 64)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)    # Remove sequence dimension\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "patience = 10\n",
    "\n",
    "all_accuracies = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tensor)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "    X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
    "    y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, criterion, optimizer, scheduler\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(torch.unique(y_tensor))\n",
    "    model = TransformerModel(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_accuracy = 0\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        y_val_pred = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_val_pred.extend(predicted.numpy())\n",
    "        val_accuracy = accuracy_score(y_val.numpy(), y_val_pred)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), f'best_model_fold{fold}.pth')\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(f\"Best Validation Accuracy for Fold {fold + 1}: {best_accuracy * 100:.2f}%\")\n",
    "    all_accuracies.append(best_accuracy)\n",
    "\n",
    "# Calculate average accuracy across folds\n",
    "average_accuracy = np.mean(all_accuracies)\n",
    "print(f\"\\nAverage Cross-Validation Accuracy: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fba26-9577-4b09-9ba4-2a060f80da47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
